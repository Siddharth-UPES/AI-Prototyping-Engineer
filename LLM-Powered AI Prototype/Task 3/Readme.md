# TASK 3 ‚Äì Rapid Iteration Challenge  
## Advanced Capability: **Multi-Document Reasoning**

---

## üéØ Goal

Enhance the existing **LLM-powered Chat with PDFs (RAG system)** by adding  
**Multi-Document Reasoning** capability.

This allows the system to:
- Accept **multiple PDFs**
- Retrieve information **across documents**
- Generate answers that synthesize knowledge from **more than one source**

---

## üöÄ Advanced Capability Chosen

### ‚úÖ Multi-Document Reasoning

Instead of limiting answers to a single PDF, the system now reasons over  
**multiple documents simultaneously** using a shared vector space.

---

## üß† Why This Capability Was Chosen

Let‚Äôs be real ‚Äî in the real world, answers don‚Äôt live in one file.

### Practical Reasons:
- Enterprise data is **distributed** (reports, policies, manuals)
- Academic research spans **multiple papers**
- Users expect **cross-document insights**, not isolated answers

### Technical Value:
- Tests the **true strength of RAG**
- Pushes retrieval + reasoning, not just generation
- Directly relevant to **knowledge bots** and **enterprise AI**

---

## ‚öôÔ∏è How It Works (High-Level)

1. User uploads **multiple PDFs**
2. All documents are:
   - Extracted
   - Chunked
   - Embedded into a **single FAISS index**
3. User asks a question
4. Retriever fetches **top-k chunks across all documents**
5. Context is merged and passed to the LLM
6. LLM generates a **document-grounded answer**

---

## üîÑ Example Scenario

**Documents Uploaded:**
- Company Policy PDF
- Technical Architecture PDF

**User Question:**
> What security rules apply to the system architecture?

**System Behavior:**
- Retrieves security rules from Policy PDF
- Retrieves architecture details from Tech PDF
- Combines both to produce a **single grounded answer**

---

## ‚öñÔ∏è Trade-Offs

| Trade-Off | Explanation |
|--------|------------|
| Increased retrieval complexity | More documents = larger vector space |
| Context window limits | Too many chunks can overflow LLM context |
| Slight latency increase | More embeddings + retrieval time |

**Reality check:**  
Power comes with cost. We manage it, not avoid it.

---

## üöß Limitations

### 1Ô∏è‚É£ Context Window Constraints
- LLMs can only process limited tokens
- Requires careful **top-k tuning**

### 2Ô∏è‚É£ Source Attribution Complexity
- Harder to clearly reference *which document said what*
- Needs future improvement (citations per chunk)

### 3Ô∏è‚É£ Scaling Challenges
- Large document collections require:
  - Persistent vector DB
  - Sharding or metadata filtering

---

## üîÆ Future Improvements

- Per-document metadata filtering
- Document-level confidence scoring
- Citation-aware responses
- Hybrid search (keyword + semantic)

---

## üß† Key Insight

Single-document QA is a demo.  
**Multi-document reasoning is production.**

This enhancement moves the system from a **toy chatbot**  
to a **real knowledge assistant** capable of handling real-world complexity.

---

## üë§ Author

**Siddharth Kumar**  
MCA (AI & ML)  
LLM & RAG Systems Engineer

